{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paperId': '62d1a3137b01a69443bebf4d92c1990ec512a6a1', 'url': 'https://www.semanticscholar.org/paper/62d1a3137b01a69443bebf4d92c1990ec512a6a1', 'title': 'Extracting Training Data from Large Language Models', 'citationCount': 402, 'influentialCitationCount': 35, 'authors': [{'authorId': '2483738', 'name': 'Nicholas Carlini'}, {'authorId': '2444919', 'name': 'Florian Tramèr'}, {'authorId': '145217343', 'name': 'Eric Wallace'}, {'authorId': '40844378', 'name': 'Matthew Jagielski'}, {'authorId': '1404060687', 'name': 'Ariel Herbert-Voss'}, {'authorId': '2119194377', 'name': 'Katherine Lee'}, {'authorId': '145625142', 'name': 'Adam Roberts'}, {'authorId': '31035595', 'name': 'Tom B. Brown'}, {'authorId': '143711382', 'name': 'D. Song'}, {'authorId': '1758110', 'name': 'Ú. Erlingsson'}, {'authorId': '3046437', 'name': 'Alina Oprea'}, {'authorId': '2402716', 'name': 'Colin Raffel'}]}\n"
     ]
    }
   ],
   "source": [
    "#import library for api calls\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#given a semantic scholar paper ID, get the authors with their affiliations\n",
    "def get_authors(paper_id):\n",
    "    request_url = 'https://api.semanticscholar.org/graph/v1/paper/{}/authors?fields=name,affiliations'.format(paper_id)\n",
    "\n",
    "    response = requests.get(request_url)\n",
    "    response_json = response.json()\n",
    "    while 'message' in response_json.keys():\n",
    "        print(response_json['message'])\n",
    "        #wait 5 minutes and try again\n",
    "        #print the time\n",
    "        print(time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "        print(\"waiting five minutes\")\n",
    "        #print the time this will resume\n",
    "        print(time.strftime(\"%H:%M:%S\", time.localtime(time.time() + 310)))\n",
    "        time.sleep(310)\n",
    "        response = requests.get(request_url)\n",
    "        response_json = response.json()\n",
    "    authors = response_json['data']\n",
    "    return authors\n",
    "\n",
    "#get_authors('62d1a3137b01a69443bebf4d92c1990ec512a6a1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to get the papers for a given year, taking string search_query and integer paper_year as input\n",
    "def get_papers(search_query: str, paper_year: int):\n",
    "    request_url = 'https://api.semanticscholar.org/graph/v1/paper/search?query=' + search_query + '&year=' + str(paper_year) + '&offset=0&limit=100&fields=url,authors,title,influentialCitationCount,citationCount'\n",
    "    response = requests.get(request_url)\n",
    "    response_json = response.json()\n",
    "\n",
    "    #print(response_json['data'])\n",
    "    return(response_json['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_range = range(2012,2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012\n",
      "100\n",
      "25, 14, 9, 1, 28, 19, 18, 6, 6, 3, 3, 2, 2, 2, 5, 1, 3, 2780, 57, 51, 28, 10, 4, 4, 2, 19, 256, 8, 8, 7, 3, 65, 1723, 53, 2, 239, 47, 947, 34, 60, 72, 24, 34, 40, 58, 49, 53, 39, 33, 21, 34, 19, 34, 28, 112, 29, 79, 79, 31, 15, 21, 73, 27, 66, 57, 56, 16, 21, 50, 20, 9, 9, 8, 9, 13, 16, 41, 12, 347, 14, 10, 3, 3, 331, 9, 8, 8, 34, 33, 4, 7, 7, 3, 5, 4, 31, 2, 2, 6, 6, \n",
      "\n",
      "1, 0, 1, 0, 1, 3, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 142, 2, 4, 2, 1, 0, 0, 0, 2, 10, 1, 2, 0, 0, 6, 117, 3, 0, 16, 5, 87, 3, 8, 2, 2, 3, 7, 4, 6, 5, 2, 0, 0, 2, 0, 0, 4, 3, 0, 8, 12, 2, 0, 4, 0, 1, 1, 2, 4, 3, 1, 2, 3, 2, 1, 0, 0, 1, 0, 4, 1, 25, 0, 0, 0, 0, 15, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2013\n",
      "100\n",
      "1, 584, 46, 36, 23, 11, 6, 8, 3, 2, 1, 2, 1, 10, 271, 953, 60, 33, 4, 6, 276, 2, 119, 224, 8, 7, 7, 6, 9, 133, 4, 91, 2, 147, 134, 48, 4, 45, 37, 44, 41, 29, 30, 136, 41, 101, 566, 30, 432, 22, 19, 16, 15, 11, 465, 26, 24, 8, 11, 59, 14, 16, 9, 6, 6, 20, 8, 39, 425, 9, 11, 13, 10, 7, 6, 39, 9, 2, 8, 32, 12, 4, 12, 10, 10, 3, 9, 9, 318, 1, 6, 4, 8, 8, 7, 3, 8, 28, 2, 4, \n",
      "\n",
      "1, 51, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 37, 131, 0, 1, 0, 0, 32, 0, 7, 19, 2, 1, 0, 1, 0, 9, 0, 9, 0, 1, 3, 1, 0, 3, 4, 6, 4, 2, 2, 1, 0, 6, 35, 2, 55, 1, 3, 1, 1, 0, 50, 1, 0, 0, 0, 2, 1, 6, 0, 0, 0, 0, 0, 4, 27, 0, 0, 3, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 15, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2014\n",
      "100\n",
      "9, 37, 34, 13, 10, 4, 3, 2, 2, 1, 1, 1, 1, 8, 2, 769, 873, 109, 489, 69, 365, 3, 412, 224, 227, 24, 5, 138, 154, 9, 9, 7, 4, 4592, 3, 2, 2, 48, 52, 88, 1, 39, 75, 43, 24, 53, 52, 47, 45, 34, 1661, 39, 25, 22, 37, 31, 21, 743, 88, 11, 17, 94, 13, 16, 69, 15, 13, 24, 62, 22, 10, 21, 19, 11, 19, 19, 382, 476, 38, 52, 16, 8, 12, 7, 39, 8, 7, 9, 34, 11, 6, 6, 6, 10, 10, 5, 5, 33, 9, 4, \n",
      "\n",
      "0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 61, 99, 33, 40, 3, 121, 0, 24, 17, 25, 0, 0, 17, 26, 2, 0, 2, 1, 461, 0, 0, 1, 1, 1, 9, 0, 5, 1, 9, 0, 4, 2, 3, 5, 1, 78, 9, 2, 0, 3, 0, 1, 27, 8, 0, 3, 4, 2, 1, 10, 0, 0, 3, 9, 2, 2, 0, 2, 0, 0, 0, 68, 32, 1, 2, 0, 0, 0, 0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 1, 1, 0, 0, 5, 2, 0, 2015\n",
      "100\n",
      "5, 2907, 169, 79, 24, 48, 14, 30, 11, 2, 2, 2, 961, 81, 516, 1582, 377, 9, 5, 309, 397, 13, 21, 8, 104, 175, 80, 56, 6, 6, 110, 3, 131, 1, 1, 56, 78, 85, 68, 89, 69, 65, 76, 69, 54, 51, 37, 38, 1735, 2148, 14, 1539, 990, 17, 29, 16, 30, 702, 31, 552, 28, 21, 21, 27, 26, 24, 17, 25, 13, 11, 783, 45, 19, 17, 50, 20, 21, 52, 324, 43, 39, 6, 4, 14, 14, 13, 13, 36, 7, 6, 35, 10, 13, 5, 14, 297, 11, 11, 45, 2, \n",
      "\n",
      "0, 789, 22, 16, 0, 3, 0, 1, 0, 0, 0, 0, 63, 2, 52, 125, 14, 0, 0, 43, 14, 0, 1, 1, 4, 10, 1, 8, 2, 0, 2, 0, 4, 0, 0, 6, 7, 6, 11, 3, 7, 6, 3, 4, 2, 0, 2, 0, 139, 249, 0, 201, 81, 1, 7, 0, 4, 90, 1, 114, 1, 3, 4, 0, 0, 0, 1, 1, 0, 0, 19, 3, 2, 2, 3, 2, 2, 0, 79, 3, 3, 0, 0, 1, 0, 0, 0, 3, 0, 0, 4, 0, 1, 0, 1, 46, 2, 3, 5, 1, 2016\n",
      "100\n",
      "13, 1, 194, 15, 14, 4, 3, 3, 13, 1, 1, 183, 1741, 291, 25, 126, 1491, 254, 882, 35, 827, 992, 7, 259, 50, 10, 46, 581, 208, 165, 11, 116, 5, 125, 160, 105, 3, 3, 82, 93, 41, 93, 58, 34, 38, 71, 69, 33, 158, 62, 62, 53, 29, 25, 25, 51, 44, 48, 46, 122, 37, 88, 27, 24, 29, 6025, 17, 7410, 31, 24, 1193, 2280, 29, 26, 24, 743, 92, 564, 22, 16, 612, 21, 11, 73, 12, 10, 375, 59, 8, 9, 16, 17, 17, 7, 15, 13, 13, 5, 13, 11, \n",
      "\n",
      "1, 0, 19, 1, 0, 0, 0, 0, 0, 0, 0, 18, 163, 12, 0, 19, 204, 37, 243, 1, 229, 130, 0, 49, 3, 1, 0, 88, 13, 11, 1, 7, 0, 26, 5, 5, 0, 0, 6, 5, 5, 3, 1, 3, 4, 9, 18, 0, 27, 4, 4, 4, 0, 3, 0, 2, 3, 2, 4, 10, 3, 7, 2, 6, 0, 1785, 0, 1048, 1, 0, 282, 137, 2, 2, 3, 97, 7, 81, 1, 4, 83, 0, 1, 6, 0, 1, 100, 5, 0, 2, 1, 0, 0, 0, 1, 2, 1, 0, 0, 0, 2017\n",
      "100\n",
      "1, 65, 13, 29, 4, 3, 5, 4, 10, 7, 2, 2, 113, 1036, 29, 18, 47, 62, 12, 49, 6, 4, 4, 3, 4, 137, 107, 68, 51, 40, 35, 19, 60, 25, 61, 23, 52, 4, 49, 18, 6, 15, 16, 28, 28, 59, 21, 8, 24, 25, 76, 22, 72, 13, 20, 46, 8, 9, 8, 19, 9, 881, 5, 5, 17, 657, 12, 13, 13, 3, 780, 5, 5, 561, 777, 12, 12, 11, 3, 3, 34, 2, 2, 10, 10, 16, 8, 1, 9, 6, 24, 23, 3, 3, 5, 5, 5, 5, 5, 294, \n",
      "\n",
      "0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 11, 115, 1, 0, 7, 2, 0, 9, 1, 0, 0, 0, 0, 23, 9, 4, 5, 5, 1, 0, 7, 1, 4, 4, 3, 0, 3, 2, 0, 2, 1, 2, 1, 1, 2, 0, 1, 0, 1, 1, 4, 2, 0, 1, 0, 0, 0, 0, 0, 160, 0, 0, 0, 138, 0, 1, 1, 0, 74, 0, 0, 173, 29, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 87, 2018\n",
      "100\n",
      "22, 111, 92, 45, 31, 32, 39, 21, 15, 30, 18, 10, 9, 6, 3, 5, 3, 2, 6, 5, 4459, 133, 38, 296, 75, 67, 258, 9, 799, 28, 273, 130, 11, 12, 12, 7, 92, 77, 16, 67, 9, 9, 86, 13, 68, 16, 6, 39, 82, 136, 5, 44, 1, 29, 93, 66, 89, 136, 86, 48, 20, 39, 266, 4, 16, 33, 30, 23, 26, 58, 57, 57, 44, 54, 21, 43, 40, 26, 24, 33, 28, 26, 8968, 24, 12, 24, 104, 10, 10, 4, 4, 22, 16, 23, 19, 20, 21, 8, 18, 8, \n",
      "\n",
      "2, 5, 6, 11, 11, 2, 2, 2, 2, 6, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 785, 17, 4, 39, 9, 4, 32, 0, 145, 0, 17, 30, 2, 0, 0, 0, 10, 9, 2, 8, 1, 0, 6, 1, 8, 1, 0, 2, 6, 5, 0, 5, 0, 3, 20, 9, 14, 17, 14, 1, 1, 3, 37, 0, 1, 0, 1, 3, 2, 2, 6, 3, 7, 1, 0, 3, 1, 3, 2, 2, 2, 1, 1509, 0, 3, 0, 6, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 3, 2, 1, 2019\n",
      "100\n",
      "1, 1341, 95, 30, 37, 222, 10, 624, 132, 46, 2537, 0, 8, 176, 36, 31, 48, 20, 9, 9, 244, 7, 6, 5, 3, 10, 6, 16, 105, 213, 631, 3494, 730, 5168, 204, 827, 415, 50, 1722, 428, 471, 14, 366, 312, 17, 222, 219, 830, 114, 190, 104, 8, 4, 9, 69, 3, 3, 34, 753, 44, 292, 25, 85, 406, 38, 331, 263, 8, 85, 42, 90, 7, 5, 130, 6, 141, 3, 24, 97, 22, 26, 51, 29, 96, 451, 116, 74, 21, 22, 102, 100, 29, 29, 9, 21, 74, 70, 50, 71, 73, \n",
      "\n",
      "0, 295, 17, 2, 7, 38, 0, 136, 25, 4, 479, 0, 0, 11, 1, 5, 8, 1, 2, 0, 67, 0, 0, 0, 0, 0, 1, 0, 19, 68, 99, 765, 62, 816, 53, 185, 114, 6, 303, 114, 64, 1, 65, 44, 1, 49, 39, 197, 17, 38, 7, 0, 0, 0, 22, 0, 0, 5, 146, 5, 44, 2, 13, 92, 3, 11, 14, 2, 10, 8, 3, 3, 1, 19, 0, 28, 0, 8, 19, 4, 2, 1, 4, 3, 108, 21, 5, 3, 0, 15, 15, 3, 0, 0, 1, 2, 19, 5, 10, 13, 2020\n",
      "100\n",
      "402, 55, 194, 7, 30, 519, 350, 12, 73, 54, 21, 21, 28, 31, 391, 18, 61, 100, 27, 32, 32, 33, 26, 56, 3, 22, 34, 34, 30, 22, 19, 7, 13, 9, 9, 7, 7, 17, 5, 4, 11, 7, 7246, 23, 366, 64, 202, 92, 513, 143, 226, 35, 32, 90, 29, 21, 179, 53, 17, 58, 8, 1031, 7, 738, 73, 12, 831, 30, 55, 23, 4, 5, 18, 176, 53, 218, 209, 47, 136, 42, 38, 34, 181, 16, 22, 13, 154, 30, 11, 11, 5, 133, 125, 93, 20, 6, 14, 4, 6, 35, \n",
      "\n",
      "35, 4, 35, 1, 2, 91, 102, 1, 14, 6, 5, 5, 0, 2, 88, 1, 8, 20, 8, 4, 8, 9, 3, 14, 0, 4, 7, 6, 7, 4, 2, 3, 1, 0, 0, 1, 0, 1, 0, 0, 2, 1, 874, 4, 96, 9, 51, 11, 71, 6, 52, 9, 3, 7, 1, 2, 25, 7, 0, 5, 1, 159, 0, 67, 17, 0, 148, 14, 10, 1, 0, 0, 0, 38, 12, 41, 54, 6, 10, 4, 8, 4, 30, 2, 6, 3, 31, 3, 1, 2, 0, 32, 31, 18, 0, 0, 1, 1, 0, 5, "
     ]
    }
   ],
   "source": [
    "#loop through years, get papers for that year, then get authors for each paper\n",
    "#return a data frame, one row for each author*paper, listing the paper's ID, author's name, and affiliation\n",
    "#return another data frame, one row for each paper, listing the paper's ID, title, year, influential citation count, and citation count\n",
    "#do this serially in order to avoid hitting the API rate limit\n",
    "\n",
    "papers_by_year_dict = {}\n",
    "for year in year_range:\n",
    "    print(year)\n",
    "    papers_for_year = get_papers('large \"language model\"', year)\n",
    "    papers_by_year_dict[year]=papers_for_year\n",
    "    print(len(papers_for_year))\n",
    "    for paper in papers_for_year:\n",
    "        print(paper['citationCount'],end=\", \")\n",
    "    print(\"\\n\")\n",
    "    for paper in papers_for_year:\n",
    "        print(paper['influentialCitationCount'],end=\", \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "40, 7, 27, 450, 52, 48, 19, 144, 15, 93, 118, 40, 3, 5, 8, 6, 5, 5, 16, 95, 22, 26, 18, 24, 16, 20, 185, 20, 17, 17, 6, 12, 14, 10, 6, 9, 6, 6, 8, 4, 3, 5, 4, 4, 5, 5, 5, 5, 4, 34, 68, 17, 1, 93, 94, 68, 24, 50, 20, 13, 4, 10, 13, 9, 16, 6, 37, 22, 33, 5, 5, 29, 11, 8, 6, 6, 78, 8, 274, 244, 49, 42, 145, 46, 11, 21, 6, 28, 39, 94, 8, 10, 40, 62, 6, 60, 58, 26, 50, 52, \n",
      "\n",
      "11, 0, 1, 129, 15, 5, 5, 32, 1, 6, 10, 4, 0, 0, 2, 0, 1, 0, 1, 14, 7, 1, 1, 0, 0, 0, 74, 2, 1, 2, 0, 3, 1, 1, 2, 0, 3, 0, 0, 0, 0, 1, 2, 2, 0, 0, 1, 0, 1, 6, 24, 1, 0, 21, 13, 12, 1, 4, 3, 3, 1, 1, 0, 0, 4, 0, 5, 8, 10, 0, 0, 0, 2, 3, 1, 0, 16, 1, 80, 26, 4, 7, 24, 5, 2, 6, 1, 6, 2, 18, 1, 0, 9, 3, 0, 10, 6, 2, 15, 9, "
     ]
    }
   ],
   "source": [
    "year = 2021\n",
    "papers_for_year = get_papers('large \"language model\"', year)\n",
    "papers_by_year_dict[year]=papers_for_year\n",
    "print(len(papers_for_year))\n",
    "for paper in papers_for_year:\n",
    "    print(paper['citationCount'],end=\", \")\n",
    "print(\"\\n\")\n",
    "for paper in papers_for_year:\n",
    "    print(paper['influentialCitationCount'],end=\", \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "2013\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "2014\n",
      "................................................Too Many Requests\n",
      "19:41:38\n",
      "waiting five minutes\n",
      "19:46:38\n",
      "....................................................\n",
      "\n",
      "\n",
      "2015\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "2016\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "2017\n",
      "..............................Too Many Requests\n",
      "19:47:37\n",
      "waiting five minutes\n",
      "19:52:37\n",
      "Too Many Requests\n",
      "19:52:38\n",
      "waiting five minutes\n",
      "19:57:38\n",
      "......................................................................\n",
      "\n",
      "\n",
      "2018\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "2019\n",
      "......................................................................................Too Many Requests\n",
      "19:59:07\n",
      "waiting five minutes\n",
      "20:04:07\n",
      "Too Many Requests\n",
      "20:04:08\n",
      "waiting five minutes\n",
      "20:09:08\n",
      "..............\n",
      "\n",
      "\n",
      "2020\n",
      "....................................................................................................\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OK, now, for each year, get the authors for each paper wit htheir affilaitions\n",
    "#let's start with just 2012\n",
    "#this will take a while.\n",
    "authors_by_paper_dict = {}\n",
    "for year in year_range:\n",
    "    print(year)\n",
    "    papers_for_year = papers_by_year_dict[year]\n",
    "    for i, paper in enumerate(papers_for_year):\n",
    "        print(\".\",end=\"\")\n",
    "        #print(paper['title'])\n",
    "        paper_authors = get_authors(paper['paperId'])\n",
    "        # if i % 10 == 0:\n",
    "        #     print(paper['title'])\n",
    "        authors_by_paper_dict[paper['paperId']]=paper_authors\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n",
      "....................................................................................................\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OK, now, for each year, get the authors for each paper wit htheir affilaitions\n",
    "#let's start with just 2012\n",
    "#this will take a while.\n",
    "#authors_by_paper_dict = {}\n",
    "year=2021\n",
    "print(year)\n",
    "papers_for_year = papers_by_year_dict[year]\n",
    "for i, paper in enumerate(papers_for_year):\n",
    "    print(\".\",end=\"\")\n",
    "    #print(paper['title'])\n",
    "    paper_authors = get_authors(paper['paperId'])\n",
    "    # if i % 10 == 0:\n",
    "    #     print(paper['title'])\n",
    "    authors_by_paper_dict[paper['paperId']]=paper_authors\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the authors_by_paper_dict to a file\n",
    "\n",
    "with open('../../data/authors_by_paper_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(authors_by_paper_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_author_affiliation_dict_to_df(authors_by_paper_dict):\n",
    "    #now let's put this into a dataframe\n",
    "    #one row for each author*paper, listing the paper's ID, author's name, and affiliation\n",
    "    author_df = pd.DataFrame(columns=['paper_id', 'author_name', 'author_affiliation'])\n",
    "    for paper_id in authors_by_paper_dict.keys():\n",
    "        authors= authors_by_paper_dict[paper_id]\n",
    "        for a_i, author in enumerate(authors):\n",
    "            #create a data frame holding the paper ID, author name, and affiliation\n",
    "            if author['affiliations'] is None or len(author['affiliations'])==0:\n",
    "                #author['affiliations'] = None\n",
    "                author_row = pd.DataFrame({'paper_id':[paper_id], 'authorId':[author['authorId']], 'author_name':[author['name']],'author_precedence':[a_i]})\n",
    "            else:\n",
    "                a_count = len(author['affiliations'])\n",
    "                # if a_count>1:\n",
    "                #     print(\"more than one affiliation\")\n",
    "                #     print(paper_id, author['name'], author['affiliations'])\n",
    "                author_row = pd.DataFrame({'paper_id':[paper_id]*a_count, 'authorId':[author['authorId']]*a_count, 'author_name':[author['name']]*a_count, 'author_affiliation':author['affiliations'],'author_precedence':[a_i]*a_count})\n",
    "            author_df = author_df.append(author_row, ignore_index=True)\n",
    "        \n",
    "            #print(paper_id, author['name'], author['affiliations'])\n",
    "\n",
    "            #print(author['name'], author['affiliations'])\n",
    "    author_df.author_precedence=author_df.author_precedence.astype(int)\n",
    "    return author_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def convert_papers_json_to_df(papers_json)\n",
    "#convert list of dictionaries to a data frame\n",
    "def convert_papers_json_to_df(papers_json):\n",
    "    papers_df = pd.DataFrame(papers_json)\n",
    "    #remove authors from the data frame\n",
    "    papers_df = papers_df.drop(columns=['authors'])    \n",
    "    return papers_df\n",
    "\n",
    "#convert the papers_by_year_dict to a data frame\n",
    "papers_df_list = []\n",
    "for year in papers_by_year_dict.keys():\n",
    "    papers_for_year = papers_by_year_dict[year]\n",
    "    papers_for_year_df = convert_papers_json_to_df(papers_for_year)\n",
    "    papers_for_year_df['year'] = year\n",
    "    papers_df_list = papers_df_list + [papers_for_year_df]\n",
    "\n",
    "papers_df = pd.concat(papers_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle papers_for_year\n",
    "with open('data/papers_by_year_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(papers_by_year_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now I think we almost have enough to put this together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df = convert_author_affiliation_dict_to_df(authors_by_paper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_affiliation</th>\n",
       "      <th>authorId</th>\n",
       "      <th>author_precedence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6226e7dcee9e050face0caf852de37a9c2937812</td>\n",
       "      <td>Aline Villavicencio</td>\n",
       "      <td>University of Sheffield, Federal University of...</td>\n",
       "      <td>145585242</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2c898abb4d7f1c1793f99632396afd9f3d149ae2</td>\n",
       "      <td>Yasuhisa Fujii</td>\n",
       "      <td>Google</td>\n",
       "      <td>1851865</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>d20744d41de25c3181f3b3fcb749531a6ec9e76d</td>\n",
       "      <td>Sandra Kübler</td>\n",
       "      <td>Indiana University</td>\n",
       "      <td>1804668</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0ce317d84086b8885ddbc7923ec00bedb64ab6dc</td>\n",
       "      <td>François Yvon</td>\n",
       "      <td>LIMSI, CNRS</td>\n",
       "      <td>1846431</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>beb1226c89615d2b092b9230c4df5c496a2a76eb</td>\n",
       "      <td>Haizhou Li</td>\n",
       "      <td>National University of Singapore</td>\n",
       "      <td>1711271</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338</th>\n",
       "      <td>26e3d58181724f9ef77973ff0f65bac06e499fec</td>\n",
       "      <td>Yu Yan</td>\n",
       "      <td>microsoft.com</td>\n",
       "      <td>145967727</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4358</th>\n",
       "      <td>c7d8b6180b52f9be891fec1223b7f561770b0206</td>\n",
       "      <td>Robert Schwarzenberg</td>\n",
       "      <td>German Research Center for Artificial Intellig...</td>\n",
       "      <td>1683203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>e1a3e6856b6ac6af3600b5954392e5368603fd1b</td>\n",
       "      <td>Huan Yang</td>\n",
       "      <td>Microsoft Research</td>\n",
       "      <td>46402216</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>b53c386b7c65af80905dc05a9b27e98e03324739</td>\n",
       "      <td>Viet Dac Lai</td>\n",
       "      <td>University of Oregon</td>\n",
       "      <td>1405279380</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>b53c386b7c65af80905dc05a9b27e98e03324739</td>\n",
       "      <td>Thien Huu Nguyen</td>\n",
       "      <td>University of Oregon</td>\n",
       "      <td>1811211</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>489 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      paper_id           author_name  \\\n",
       "76    6226e7dcee9e050face0caf852de37a9c2937812   Aline Villavicencio   \n",
       "84    2c898abb4d7f1c1793f99632396afd9f3d149ae2        Yasuhisa Fujii   \n",
       "93    d20744d41de25c3181f3b3fcb749531a6ec9e76d         Sandra Kübler   \n",
       "150   0ce317d84086b8885ddbc7923ec00bedb64ab6dc         François Yvon   \n",
       "172   beb1226c89615d2b092b9230c4df5c496a2a76eb            Haizhou Li   \n",
       "...                                        ...                   ...   \n",
       "4338  26e3d58181724f9ef77973ff0f65bac06e499fec                Yu Yan   \n",
       "4358  c7d8b6180b52f9be891fec1223b7f561770b0206  Robert Schwarzenberg   \n",
       "4365  e1a3e6856b6ac6af3600b5954392e5368603fd1b             Huan Yang   \n",
       "4369  b53c386b7c65af80905dc05a9b27e98e03324739          Viet Dac Lai   \n",
       "4371  b53c386b7c65af80905dc05a9b27e98e03324739      Thien Huu Nguyen   \n",
       "\n",
       "                                     author_affiliation    authorId  \\\n",
       "76    University of Sheffield, Federal University of...   145585242   \n",
       "84                                               Google     1851865   \n",
       "93                                   Indiana University     1804668   \n",
       "150                                         LIMSI, CNRS     1846431   \n",
       "172                    National University of Singapore     1711271   \n",
       "...                                                 ...         ...   \n",
       "4338                                      microsoft.com   145967727   \n",
       "4358  German Research Center for Artificial Intellig...     1683203   \n",
       "4365                                 Microsoft Research    46402216   \n",
       "4369                               University of Oregon  1405279380   \n",
       "4371                               University of Oregon     1811211   \n",
       "\n",
       "      author_precedence  \n",
       "76                    0  \n",
       "84                    4  \n",
       "93                    0  \n",
       "150                   2  \n",
       "172                   2  \n",
       "...                 ...  \n",
       "4338                  2  \n",
       "4358                  1  \n",
       "4365                  5  \n",
       "4369                  1  \n",
       "4371                  3  \n",
       "\n",
       "[489 rows x 5 columns]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print authors with affiliations\n",
    "author_df[author_df['author_affiliation'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def code_affiliation(affiliation):\n",
    "    #check if affiliation contains the word 'University'\n",
    "    #do a fuzzy match using fuzzywuzzy on whether affiliation contains 'University'\n",
    "    #if it does, return 'Academic'\n",
    "    \n",
    "    #fuzzywuzzy match\n",
    "    #print(affiliation)\n",
    "    if affiliation is np.nan:\n",
    "        return affiliation\n",
    "\n",
    "    al=affiliation.lower()\n",
    "    \n",
    "    if fuzz.partial_ratio(\"university\",al)>=80:\n",
    "        return 'Academic'\n",
    "    if fuzz.partial_ratio(\"Institute of Technology\".lower(),affiliation)>=90:\n",
    "        return 'Academic'\n",
    "    misc_academic_names = [\n",
    "        \"Univ.\",\"Nara Institute of Science and Technology (NAIST)\",\n",
    "         \"Univ. Grenoble Alpes\",\"MIT\",\"LMU Munich\",\"UCLA\",\"UNC Chapel Hill\",\"Georgia Tech\",\n",
    "         \"Stanford\",\"Brooklyn College\"]\n",
    "    #check to see if any of the misc academic names are in the affiliation\n",
    "    for name in misc_academic_names:\n",
    "        if name.lower() in al:\n",
    "            return 'Academic'\n",
    "\n",
    "    #now do corporate affiliations\n",
    "    corporate_fuzzy_matches = [\n",
    "        \"Google\",\"Facebook\",\"Microsoft\",\"DeepMind\",\"Meta AI\",\n",
    "        \"Amazon\",\"IBM\",\"Apple\",\"Baidu\",\"Alibaba\",\"JD.com\",\"Tencent\",\n",
    "        \"Huawei\",\"Salesforce\"\n",
    "    ]\n",
    "    for cfm in corporate_fuzzy_matches:\n",
    "        if fuzz.partial_ratio(cfm,affiliation)>=90:\n",
    "            return 'For-profit corporation'\n",
    "\n",
    "    #non-profit institutes\n",
    "    nonprofit_fuzzy_matches = [\"OpenAI\",\"Allen Institute\"]\n",
    "    for fuzzy_match_str in nonprofit_fuzzy_matches:\n",
    "        if fuzz.partial_ratio(fuzzy_match_str,affiliation)>=90:\n",
    "            return 'Non-profit organization'\n",
    "\n",
    "\n",
    "    return 'Undetermined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df['affiliation_type'] = [code_affiliation(aa) for aa in author_df.author_affiliation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Academic                   263\n",
       "For-profit corporation     116\n",
       "Undetermined                84\n",
       "Non-profit organization     26\n",
       "Name: affiliation_type, dtype: int64"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_df['affiliation_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Academic                   82\n",
       "Undetermined               21\n",
       "For-profit corporation     20\n",
       "Non-profit organization     7\n",
       "Name: affiliation_type, dtype: int64"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_df.loc[author_df['author_precedence']==0,'affiliation_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inria    3\n",
      "Name: author_affiliation, dtype: int64\n",
      "appearing twice:\n",
      "EPFL, Waymo, Samsung Electronics, ETH Zürich, UC Davis, Department of Computer Science, CMU LTI, Samsung Research, National Institute of Information & Communications Technology (NICT), L3S Research Center, NICT\n",
      "appearing once\n",
      "ASAPP Inc, FAIR, Symanto, Square Inc, Adobe, ETH Zurich, Qualcomm, Reveal, Technion, NHK, Inflection AI, HuggingFace Inc., Adobe Research, UC Berkeley, Hasso Plattner Institute, CRIM, UMASS Amherst, MDC Berlin, Qatar Computing Research Institute, ZBW - Leibniz Information Centre for Economics, Fluent.ai Inc., Sulzer GmbH, Advanced Digital Sciences Center Singapore, Truveta, German Research Center for Artificial Intelligence (DFKI), Nuance Communications Inc., Seagate, Worcester Polytechnic Institute, National Institute of Advanced Industrial Science and Technology, ObEN, Idiap Research Institute, Ex: Uber Technologies Inc, Honda Research Institute Japan, rippleAI, KAIST LK Lab, École de Technologie Supérieure, NYU, Mila, Institute of Automation, Chinese Academy of Sciences, Nvidia, NAVER AI LAB, USTC & MSRA, Almawave, CUNY Graduate Center, Celonis, Toyota Technological Institute at Chicago, LIMSI, CNRS, NextG, FPT Software, Alana AI, Institute of Automation Chinese Academy of Sciences, EleutherAI, Argonne National Laboratory, microsoft.com, Intel, ISCAS, CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, AIST, Fraunhofer FIT, ARL\n"
     ]
    }
   ],
   "source": [
    "#get other affiliations\n",
    "other_affiliations = author_df['author_affiliation'][author_df['affiliation_type']=='Undetermined']\n",
    "#sort by frequency, then print in that order\n",
    "sorted_affiliations = other_affiliations.value_counts().sort_values(ascending=False)\n",
    "print(sorted_affiliations[sorted_affiliations>2])\n",
    "print(\"appearing twice:\")\n",
    "print(\", \".join(sorted_affiliations[sorted_affiliations==2].index))\n",
    "print(\"appearing once\")\n",
    "print(\", \".join(sorted_affiliations[sorted_affiliations==1].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code these manually in a spreadsheet, then come back to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a format comaptible in microsoft excel\n",
    "author_df.to_csv(\"data/affiliations.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df.to_csv(\"data/papers_all_df.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now determine whether the author affiliation is a university or not. with only 416 values overall this will be best coded manually in a sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more data\n",
    "\n",
    "We're going to try a few more search terms to widen our dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_by_year_multiterm_dict = {}\n",
    "papers_by_year_multiterm_dict['large \"language model\"'] = papers_by_year_dict\n",
    "authors_by_paper_by_term_dict = {}\n",
    "authors_by_paper_by_term_dict['large \"language model\"'] = authors_by_paper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012\n",
      "100\n",
      "5, 9593, 614, 7769, 504, 1561, 245, 153, 170, 193, 102, 186, 126, 87, 167, 39, 3596, 50, 53, 62, 29, 30, 30, 3215, 73, 23, 24, 38, 24, 20, 30, 13, 6, 7, 11, 9, 22, 6, 8, 6, 7, 4, 5, 3, 3, 3, 19, 16, 16, 2, 2, 7, 14, 1, 16, 3, 3, 6, 2, 5, 3, 2, 3, 2, 0, 0, 424, 404, 418, 1825, 184, 202, 146, 110, 194, 1262, 80, 73, 45, 52, 61, 800, 40, 837, 66, 23, 32, 31, 14, 27, 17, 619, 36, 12, 12, 8, 6, 7, 4, 9, \n",
      "\n",
      "0, 527, 49, 989, 33, 127, 18, 9, 6, 13, 6, 9, 5, 2, 13, 4, 172, 3, 2, 3, 5, 0, 0, 323, 1, 0, 0, 2, 1, 0, 1, 0, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 38, 23, 46, 149, 31, 15, 5, 11, 7, 43, 15, 7, 4, 3, 2, 37, 1, 192, 5, 2, 2, 4, 0, 3, 1, 47, 3, 0, 0, 0, 1, 1, 0, 0, 2013\n",
      "100\n",
      "3899, 8021, 1271, 1094, 744, 498, 705, 614, 723, 593, 1994, 311, 1601, 386, 374, 353, 334, 462, 374, 192, 1403, 157, 212, 281, 154, 158, 252, 88, 151, 70, 47, 141, 53, 41, 48, 57, 36, 82, 101, 77, 35, 114, 136, 35, 75, 81, 60, 42, 33, 34, 100, 32, 29, 28, 44, 28, 25, 22, 47, 47, 36, 6, 915, 4, 33, 15, 14, 15, 32, 28, 7, 11, 12, 11, 8, 6, 6, 6, 8, 10, 5, 4, 15, 20, 25, 3, 3, 3, 3, 3, 970, 7, 3, 9, 7, 15, 9, 14, 1, 0, \n",
      "\n",
      "440, 1291, 137, 69, 71, 13, 53, 46, 21, 21, 237, 38, 271, 15, 17, 10, 10, 36, 16, 17, 107, 20, 7, 30, 6, 2, 9, 3, 10, 3, 5, 8, 2, 3, 3, 0, 1, 4, 6, 6, 1, 3, 4, 0, 10, 4, 2, 3, 4, 1, 2, 1, 0, 2, 1, 0, 2, 2, 2, 1, 3, 1, 109, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 26, 1, 0, 2, 0, 1, 0, 0, 0, 0, 2014\n",
      "100\n",
      "5310, 12957, 2812, 3705, 5382, 2002, 1391, 1652, 1781, 1527, 0, 1257, 519, 461, 988, 398, 877, 4212, 796, 168, 515, 2717, 924, 716, 636, 6790, 2139, 664, 119, 325, 93, 393, 393, 335, 169, 371, 312, 354, 1436, 274, 286, 40, 6006, 164, 390, 333, 232, 209, 200, 382, 150, 154, 168, 176, 147, 130, 169, 348, 162, 110, 232, 65, 83, 16296, 93, 81, 86, 78, 136, 109, 67, 69, 66, 54, 115, 148, 53, 54, 56, 55, 53, 45, 50, 43, 49, 162, 153, 44, 40, 40, 43, 42, 45, 8, 1251, 37, 39, 38, 66, 37, \n",
      "\n",
      "1500, 451, 138, 693, 919, 177, 168, 143, 115, 140, 0, 111, 92, 40, 83, 41, 30, 718, 41, 3, 39, 404, 25, 80, 30, 1558, 276, 26, 7, 50, 1, 34, 22, 30, 15, 16, 21, 12, 93, 22, 15, 4, 1245, 4, 49, 4, 14, 8, 13, 8, 31, 14, 13, 12, 17, 12, 6, 15, 5, 3, 7, 7, 6, 1458, 4, 4, 1, 4, 10, 8, 3, 4, 1, 7, 9, 4, 5, 5, 2, 4, 4, 3, 4, 4, 2, 6, 6, 1, 2, 2, 2, 1, 0, 2, 126, 2, 2, 2, 3, 0, 2015\n",
      "100\n",
      "61254, 5456, 2927, 115845, 8032, 11022, 85, 5996, 4273, 18535, 4151, 2132, 1938, 1303, 10090, 1526, 1395, 33229, 1644, 3355, 1191, 4235, 2415, 1578, 828, 4366, 6352, 963, 810, 771, 504, 427, 558, 528, 681, 566, 515, 453, 476, 525, 575, 408, 47, 1924, 496, 405, 401, 393, 394, 353, 342, 156, 185, 376, 152, 144, 129, 114, 210, 351, 302, 308, 261, 278, 263, 321, 311, 124, 323, 291, 339, 258, 286, 195, 253, 257, 281, 183, 250, 239, 83, 231, 53, 60, 199, 211, 218, 200, 68, 187, 179, 235, 177, 193, 327, 175, 167, 43, 142, 137, \n",
      "\n",
      "2853, 988, 405, 25647, 1859, 1711, 2, 1233, 843, 2955, 560, 117, 188, 297, 1497, 175, 147, 1868, 108, 642, 134, 1015, 424, 34, 134, 776, 717, 77, 107, 76, 60, 61, 52, 57, 50, 45, 72, 54, 24, 16, 48, 57, 1, 450, 20, 18, 17, 15, 12, 48, 42, 10, 6, 33, 1, 11, 11, 12, 34, 10, 24, 18, 22, 24, 23, 16, 16, 6, 12, 13, 71, 21, 10, 15, 8, 10, 7, 13, 6, 10, 2, 11, 6, 6, 20, 8, 10, 14, 3, 7, 10, 4, 14, 6, 12, 10, 10, 3, 11, 8, 2016\n",
      "100\n",
      "7741, 8298, 4199, 3857, 3160, 2283, 2306, 6040, 1968, 3336, 4366, 83, 6099, 1589, 1357, 11199, 138, 1359, 48, 1666, 692, 744, 702, 558, 1025, 115, 456, 1316, 329, 471, 1243, 86, 1113, 1195, 396, 69, 242, 2830, 1495, 370, 150, 286, 295, 9011, 641, 93, 280, 183, 703, 121, 1544, 690, 266, 195, 210, 198, 186, 733, 444, 471, 88, 948, 811, 41, 0, 791, 153, 260, 560, 486, 1100, 152, 650, 734, 385, 410, 442, 705, 731, 155, 521, 555, 453, 130, 581, 473, 446, 520, 232, 290, 380, 370, 649, 334, 372, 2933, 376, 113, 107, 736, \n",
      "\n",
      "2268, 1426, 120, 355, 742, 137, 339, 1786, 198, 137, 876, 2, 1169, 91, 145, 1442, 6, 74, 2, 82, 118, 58, 99, 111, 36, 20, 51, 32, 31, 42, 29, 1, 177, 59, 24, 4, 22, 381, 30, 21, 8, 40, 19, 1360, 17, 7, 10, 18, 62, 10, 269, 57, 12, 15, 9, 8, 10, 49, 46, 43, 12, 30, 39, 1, 0, 83, 17, 27, 68, 36, 53, 7, 27, 33, 36, 33, 35, 23, 17, 6, 23, 25, 21, 9, 12, 10, 50, 71, 14, 10, 21, 26, 60, 98, 23, 87, 51, 6, 6, 44, 2017\n",
      "100\n",
      "6601, 7179, 2777, 2116, 2379, 5348, 6664, 411, 1458, 319, 1107, 503, 1114, 553, 1210, 1756, 169, 1478, 1274, 144, 1717, 949, 136, 891, 1506, 1119, 1308, 638, 942, 1205, 53, 1008, 528, 852, 1931, 426, 777, 376, 1172, 1498, 1502, 309, 153, 361, 1233, 890, 438, 814, 143, 915, 437, 241, 717, 644, 552, 318, 565, 198, 1118, 1117, 838, 915, 1100, 688, 948, 344, 705, 804, 927, 268, 794, 472, 429, 594, 657, 545, 873, 402, 395, 549, 369, 518, 174, 420, 578, 439, 999, 924, 408, 513, 569, 387, 374, 381, 293, 521, 108, 399, 283, 623, \n",
      "\n",
      "2334, 159, 391, 69, 493, 1333, 1827, 57, 292, 9, 44, 114, 73, 66, 204, 216, 10, 141, 138, 4, 56, 53, 3, 45, 83, 116, 44, 31, 77, 71, 4, 68, 94, 46, 39, 82, 45, 45, 34, 35, 17, 44, 29, 22, 18, 149, 40, 144, 28, 16, 127, 6, 30, 36, 36, 36, 115, 10, 34, 38, 109, 68, 42, 136, 56, 27, 97, 96, 44, 26, 79, 76, 78, 65, 63, 52, 72, 57, 58, 47, 50, 34, 0, 27, 40, 39, 36, 32, 84, 23, 13, 18, 14, 19, 18, 64, 3, 70, 8, 46, 2018\n",
      "100\n",
      "2129, 2212, 1865, 1862, 1378, 1433, 463, 175, 110, 1655, 1368, 1273, 1003, 1270, 458, 899, 863, 1288, 397, 373, 213, 197, 658, 1150, 512, 226, 229, 720, 889, 214, 372, 894, 975, 918, 1267, 3642, 96, 98, 92, 106, 639, 815, 1500, 1149, 83, 1382, 52, 1146, 853, 1212, 349, 411, 669, 1074, 368, 706, 510, 414, 373, 584, 203, 727, 381, 474, 370, 88, 406, 978, 775, 341, 417, 75, 1631, 527, 335, 359, 252, 256, 506, 379, 419, 395, 327, 293, 317, 286, 414, 254, 277, 241, 244, 251, 224, 219, 1007, 156, 210, 554, 325, 196, \n",
      "\n",
      "89, 160, 203, 208, 107, 56, 23, 16, 7, 45, 46, 117, 167, 57, 45, 52, 222, 45, 21, 17, 9, 11, 78, 158, 103, 43, 43, 30, 138, 46, 53, 187, 45, 44, 27, 987, 3, 15, 13, 16, 176, 169, 23, 69, 6, 21, 5, 88, 72, 8, 92, 83, 67, 55, 68, 51, 55, 56, 58, 56, 35, 50, 46, 48, 43, 5, 31, 31, 39, 40, 94, 2, 267, 47, 61, 54, 45, 42, 20, 22, 11, 23, 15, 20, 25, 23, 3, 9, 9, 15, 18, 3, 5, 15, 24, 10, 47, 38, 30, 42, 2019\n",
      "100\n",
      "18710, 3705, 3340, 566, 630, 368, 1459, 148, 174, 242, 177, 1440, 98, 485, 397, 1283, 337, 163, 191, 136, 142, 121, 106, 504, 86, 1335, 81, 81, 65, 98, 39, 503, 77, 63, 44, 322, 32, 886, 819, 533, 314, 347, 312, 321, 806, 255, 250, 1252, 202, 175, 167, 450, 319, 162, 292, 498, 349, 174, 151, 153, 403, 147, 755, 195, 154, 320, 351, 337, 338, 319, 156, 231, 224, 219, 167, 143, 134, 135, 133, 134, 128, 109, 118, 119, 118, 111, 106, 107, 80, 70, 71, 69, 78, 543, 472, 119, 113, 55, 46, 695, \n",
      "\n",
      "2010, 113, 341, 28, 24, 23, 38, 8, 4, 22, 5, 86, 3, 60, 18, 279, 41, 55, 10, 5, 10, 2, 2, 84, 4, 29, 8, 3, 3, 8, 4, 32, 1, 0, 1, 36, 1, 51, 39, 15, 8, 5, 19, 19, 69, 17, 19, 33, 4, 5, 6, 82, 71, 5, 50, 39, 86, 2, 16, 15, 65, 9, 70, 12, 23, 3, 7, 9, 25, 17, 4, 16, 10, 23, 5, 3, 4, 7, 13, 20, 11, 9, 18, 18, 7, 25, 21, 6, 10, 3, 6, 8, 8, 32, 35, 2, 2, 8, 10, 26, 2020\n",
      "100\n",
      "995, 1588, 147, 520, 972, 293, 353, 209, 257, 153, 203, 117, 134, 139, 110, 105, 103, 728, 102, 98, 365, 175, 450, 235, 145, 157, 180, 110, 103, 88, 86, 85, 86, 76, 82, 80, 77, 70, 485, 100, 101, 99, 59, 51, 49, 36, 76, 539, 495, 477, 30, 91, 279, 260, 176, 85, 73, 80, 315, 67, 61, 80, 73, 66, 65, 61, 98, 60, 680, 57, 495, 363, 50, 51, 179, 44, 40, 41, 35, 34, 305, 353, 67, 70, 304, 76, 521, 63, 24, 23, 136, 50, 48, 124, 36, 591, 36, 52, 50, 52, \n",
      "\n",
      "57, 61, 31, 26, 105, 18, 6, 18, 4, 12, 5, 4, 4, 4, 3, 24, 12, 54, 3, 5, 63, 56, 17, 47, 6, 1, 2, 2, 1, 9, 4, 9, 5, 7, 6, 4, 12, 3, 47, 10, 2, 1, 5, 3, 3, 3, 19, 33, 34, 33, 3, 0, 33, 31, 34, 0, 2, 1, 28, 2, 3, 6, 0, 0, 0, 1, 1, 2, 26, 1, 27, 26, 2, 1, 26, 0, 2, 2, 2, 1, 29, 35, 2, 0, 30, 1, 54, 0, 1, 1, 29, 0, 0, 38, 0, 26, 1, 1, 1, 2, 2021\n",
      "100\n",
      "601, 341, 564, 178, 325, 203, 226, 178, 162, 153, 168, 118, 113, 116, 114, 108, 108, 100, 145, 151, 176, 244, 166, 149, 119, 126, 133, 133, 143, 142, 135, 121, 140, 116, 110, 111, 117, 97, 109, 105, 108, 103, 92, 94, 94, 93, 86, 85, 83, 84, 77, 80, 81, 79, 73, 81, 64, 70, 72, 62, 66, 70, 101, 57, 54, 55, 47, 46, 46, 48, 46, 45, 45, 144, 44, 44, 95, 110, 42, 42, 41, 42, 42, 39, 37, 35, 34, 34, 36, 97, 32, 33, 31, 33, 31, 31, 32, 59, 105, 83, \n",
      "\n",
      "26, 27, 32, 28, 18, 3, 14, 11, 5, 9, 22, 8, 4, 6, 3, 7, 8, 14, 14, 15, 12, 4, 5, 2, 4, 7, 12, 4, 5, 12, 1, 2, 1, 1, 1, 2, 0, 3, 0, 1, 5, 1, 7, 3, 4, 4, 18, 4, 3, 5, 3, 4, 5, 7, 6, 7, 3, 7, 3, 7, 3, 6, 3, 17, 6, 4, 4, 4, 4, 3, 6, 3, 4, 1, 3, 3, 6, 1, 4, 4, 8, 3, 3, 5, 3, 3, 3, 5, 3, 8, 6, 4, 3, 5, 4, 3, 8, 8, 1, 21, "
     ]
    }
   ],
   "source": [
    "#loop through years, get papers for that year, then get authors for each paper\n",
    "#return a data frame, one row for each author*paper, listing the paper's ID, author's name, and affiliation\n",
    "#return another data frame, one row for each paper, listing the paper's ID, title, year, influential citation count, and citation count\n",
    "#do this serially in order to avoid hitting the API rate limit\n",
    "\n",
    "\n",
    "\n",
    "term='deep learning'\n",
    "papers_by_year_for_term_dict = {}\n",
    "for year in year_range:\n",
    "    print(year)\n",
    "    papers_for_year = get_papers(term, year)\n",
    "    papers_by_year_for_term_dict[year]=papers_for_year\n",
    "    print(len(papers_for_year))\n",
    "    for paper in papers_for_year:\n",
    "        print(paper['citationCount'],end=\", \")\n",
    "    print(\"\\n\")\n",
    "    for paper in papers_for_year:\n",
    "        print(paper['influentialCitationCount'],end=\", \")\n",
    "    \n",
    "papers_by_year_multiterm_dict[term]=papers_by_year_for_term_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_already_seen = papers_df['paperId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012\n",
      ".Too Many Requests\n",
      "17:30:10\n",
      "waiting five minutes\n",
      "17:35:20\n",
      "*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*\n",
      "\n",
      "\n",
      "2013\n",
      ".*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*\n",
      "\n",
      "\n",
      "2014\n",
      ".*.*.*.Too Many Requests\n",
      "17:36:15\n",
      "waiting five minutes\n",
      "17:41:25\n",
      "*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*\n",
      "\n",
      "\n",
      "2015\n",
      ".*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*\n",
      "\n",
      "\n",
      "2016\n",
      ".*.*.*.*.*.already seen this paper\n",
      ".*.already seen this paper\n",
      ".*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.Too Many Requests\n",
      "17:42:24\n",
      "waiting five minutes\n",
      "17:47:34\n",
      "*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*\n",
      "\n",
      "\n",
      "2017\n",
      ".*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*\n",
      "\n",
      "\n",
      "2018\n",
      ".*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.Too Many Requests\n",
      "17:48:45\n",
      "waiting five minutes\n",
      "17:53:55\n",
      "*.*.*\n",
      "\n",
      "\n",
      "2019\n",
      ".*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*\n",
      "\n",
      "\n",
      "2020\n",
      ".*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*\n",
      "\n",
      "\n",
      "2021\n",
      ".*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.Too Many Requests\n",
      "17:54:54\n",
      "waiting five minutes\n",
      "18:00:04\n",
      "Too Many Requests\n",
      "18:00:05\n",
      "waiting five minutes\n",
      "18:05:15\n",
      "*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*.*\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "term='deep learning'\n",
    "papers_for_this_term_dict = papers_by_year_multiterm_dict[term]\n",
    "#OK, now, for each year, get the authors for each paper with their affilaitions\n",
    "#let's start with just 2012\n",
    "#this will take a while.\n",
    "authors_by_paper_for_term_dict = {}\n",
    "for year in year_range:\n",
    "    print(year)\n",
    "    papers_for_year = papers_for_this_term_dict[year]\n",
    "    for i, paper in enumerate(papers_for_year):\n",
    "        print(\".\",end=\"\")\n",
    "        #print(paper['title'])\n",
    "        cur_paper_id=paper['paperId']\n",
    "        if cur_paper_id in papers_already_seen:\n",
    "            print(\"already seen this paper\")\n",
    "            continue\n",
    "        \n",
    "        paper_authors = get_authors(cur_paper_id)\n",
    "        print(\"*\",end=\"\")\n",
    "        # if i % 10 == 0:\n",
    "        #     print(paper['title'])\n",
    "        authors_by_paper_for_term_dict[paper['paperId']]=paper_authors\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "authors_by_paper_by_term_dict[term]=authors_by_paper_for_term_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierarchial design is a bit confusing, but it means I don't have to go back and re-run all the requests each time I want to add more data to the set, and can re-use the code I wrote when I just had a single search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large \"language model\"\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "deep learning\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n"
     ]
    }
   ],
   "source": [
    "#def convert_papers_json_to_df(papers_json)\n",
    "#convert list of dictionaries to a data frame\n",
    "def convert_papers_json_to_df(papers_json):\n",
    "    papers_df = pd.DataFrame(papers_json)\n",
    "    #remove authors from the data frame\n",
    "    papers_df = papers_df.drop(columns=['authors'])    \n",
    "    return papers_df\n",
    "\n",
    "#convert the papers_by_year_dict to a data frame\n",
    "papers_df_list = []\n",
    "author_df_list = []\n",
    "for term in papers_by_year_multiterm_dict.keys():\n",
    "    print(term)\n",
    "    papers_by_year_dict = papers_by_year_multiterm_dict[term]\n",
    "    #convert papers hierarchical dict to a data frame\n",
    "    for year in papers_by_year_dict.keys():\n",
    "        print(year)\n",
    "        papers_for_year = papers_by_year_dict[year]\n",
    "        papers_for_year_df = convert_papers_json_to_df(papers_for_year)\n",
    "        papers_for_year_df['year'] = year\n",
    "        papers_for_year_df['term'] = term\n",
    "        papers_df_list = papers_df_list + [papers_for_year_df]\n",
    "    #convert authors hierarchical dict to a data frame\n",
    "    author_df_list = author_df_list + [convert_author_affiliation_dict_to_df(authors_by_paper_by_term_dict[term])]\n",
    "\n",
    "papers_df = pd.concat(papers_df_list, ignore_index=True)\n",
    "author_df = pd.concat(author_df_list, ignore_index=True)\n",
    "\n",
    "author_df['affiliation_type'] = [code_affiliation(aa) for aa in author_df.author_affiliation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a format comaptible in microsoft excel\n",
    "author_df.to_csv(\"data/affiliations_extended.csv\",encoding='utf-8')\n",
    "papers_df.to_csv(\"data/papers_all_df_extended.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the authors_by_paper_dict to a file\n",
    "\n",
    "with open('data/authors_by_paper_dict_extended.pickle', 'wb') as handle:\n",
    "    pickle.dump(authors_by_paper_by_term_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#pickle papers_for_year\n",
    "with open('data/papers_by_year_dict_extended.pickle', 'wb') as handle:\n",
    "    pickle.dump(papers_by_year_multiterm_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f24b9b07534bd308380b91236ff0deadd6a2676eca81f222064df21bebc5915"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
